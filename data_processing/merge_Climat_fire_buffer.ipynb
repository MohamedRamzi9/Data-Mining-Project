{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08503215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lon       lat  fire\n",
      "0   9.48947  31.49290     1\n",
      "1   9.49053  31.49524     1\n",
      "2   9.49368  31.49449     1\n",
      "3   9.49154  31.49420     1\n",
      "4  10.09115  36.93407     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "#path_fire = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\algeria_tunisia.csv\"\n",
    "#df_fire = pd.read_csv(path_fire)\n",
    "df_fire_clean = pd.read_parquet(r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\local_dataset\\dataset\\fire_full.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "#df_fire_clean = df_fire[['longitude', 'latitude', 'type']].copy()\n",
    "\n",
    "print(df_fire_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7102209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans df_fire_clean : 56864\n",
      "Shape du dataset : (56864, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de lignes dans df_fire_clean : {len(df_fire_clean)}\")\n",
    "print(f\"Shape du dataset : {df_fire_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcf82e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_fire_clean[\\'fire\\'] = np.where(df_fire_clean[\\'type\\'] == 0, 1, 0)\\n\\nprint(f\"df_fire_clean avec colonne \\'fire\\' créé : {df_fire_clean.shape}\")\\nprint(df_fire_clean.head())'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer l'attribut binaire 'fire'\n",
    "# Si 'type' == 0, 'fire' = 1, sinon 'fire' = 0\n",
    "\"\"\"df_fire_clean['fire'] = np.where(df_fire_clean['type'] == 0, 1, 0)\n",
    "\n",
    "print(f\"df_fire_clean avec colonne 'fire' créé : {df_fire_clean.shape}\")\n",
    "print(df_fire_clean.head())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad3645",
   "metadata": {},
   "source": [
    "## Chargement et agrégation des données climatiques par saisons + Conversion en DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb821f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV climat chargé : (148292, 6)\n",
      "\n",
      "Traitement de la variable : log_precip\n",
      "  → log_precip_s1 agrégé (20301 points)\n",
      "  → log_precip_s2 agrégé (20231 points)\n",
      "  → log_precip_s3 agrégé (16200 points)\n",
      "\n",
      "Traitement de la variable : log_precip\n",
      "  → log_precip_s1 agrégé (20301 points)\n",
      "  → log_precip_s2 agrégé (20231 points)\n",
      "  → log_precip_s3 agrégé (16200 points)\n",
      "  → log_precip_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : tmax\n",
      "  → tmax_s1 agrégé (20301 points)\n",
      "  → tmax_s2 agrégé (20231 points)\n",
      "  → tmax_s3 agrégé (16200 points)\n",
      "  → tmax_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : amplitude_thermique\n",
      "  → amplitude_thermique_s1 agrégé (20301 points)\n",
      "  → amplitude_thermique_s2 agrégé (20231 points)\n",
      "  → log_precip_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : tmax\n",
      "  → tmax_s1 agrégé (20301 points)\n",
      "  → tmax_s2 agrégé (20231 points)\n",
      "  → tmax_s3 agrégé (16200 points)\n",
      "  → tmax_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : amplitude_thermique\n",
      "  → amplitude_thermique_s1 agrégé (20301 points)\n",
      "  → amplitude_thermique_s2 agrégé (20231 points)\n",
      "  → amplitude_thermique_s3 agrégé (16200 points)\n",
      "  → amplitude_thermique_s4 agrégé (21540 points)\n",
      "\n",
      " Toutes les variables saisonnières climatiques ont été générées.\n",
      "  → amplitude_thermique_s3 agrégé (16200 points)\n",
      "  → amplitude_thermique_s4 agrégé (21540 points)\n",
      "\n",
      " Toutes les variables saisonnières climatiques ont été générées.\n"
     ]
    }
   ],
   "source": [
    "climate_csv_path = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\climat_clean.csv\"\n",
    "climate_df = pd.read_csv(climate_csv_path)\n",
    "\n",
    "print(\"CSV climat chargé :\", climate_df.shape)\n",
    "\n",
    "# Convertir la date\n",
    "climate_df[\"time\"] = pd.to_datetime(climate_df[\"time\"])\n",
    "\n",
    "seasons_indices = {\n",
    "    's1': [12, 1, 2],   # Hiver (Dec-Jan-Feb)\n",
    "    's2': [3, 4, 5],    # Printemps\n",
    "    's3': [6, 7, 8],    # Été\n",
    "    's4': [9, 10, 11]   # Automne\n",
    "}\n",
    "\n",
    "# Ajouter colonne \"season\"\n",
    "\n",
    "def get_season(date):\n",
    "    m = date.month\n",
    "    if m in [12, 1, 2]:\n",
    "        return \"s1\"\n",
    "    if m in [3, 4, 5]:\n",
    "        return \"s2\"\n",
    "    if m in [6, 7, 8]:\n",
    "        return \"s3\"\n",
    "    return \"s4\"\n",
    "\n",
    "climate_df[\"season\"] = climate_df[\"time\"].apply(get_season)\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "variables = [\"log_precip\",  \"tmax\",\"amplitude_thermique\"]\n",
    "\n",
    "for var in variables:\n",
    "    print(f\"\\nTraitement de la variable : {var}\")\n",
    "\n",
    "    for season in [\"s1\", \"s2\", \"s3\", \"s4\"]:\n",
    "        \n",
    "        df_season = climate_df[climate_df[\"season\"] == season]\n",
    "\n",
    "        if df_season.empty:\n",
    "            print(f\"   Saison {season} vide pour {var}\")\n",
    "            continue\n",
    "\n",
    "        # Agrégation\n",
    "        if var == \"log_precip\":\n",
    "            df_agg = (\n",
    "                df_season.groupby([\"longitude\", \"latitude\"])[var]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={var: f\"{var}_{season}\"})\n",
    "            )\n",
    "        else:\n",
    "            df_agg = (\n",
    "                df_season.groupby([\"longitude\", \"latitude\"])[var]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={var: f\"{var}_{season}\"})\n",
    "            )\n",
    "\n",
    "        all_dfs.append(df_agg)\n",
    "\n",
    "        print(f\"  → {var}_{season} agrégé ({df_agg.shape[0]} points)\")\n",
    "\n",
    "\n",
    "print(\"\\n Toutes les variables saisonnières climatiques ont été générées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94a4d3",
   "metadata": {},
   "source": [
    "## Fusion des DataFrames climatiques en un seul DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9043fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Fusion 1/11 terminée\n",
      "   → Fusion 2/11 terminée\n",
      "   → Fusion 3/11 terminée\n",
      "   → Fusion 4/11 terminée\n",
      "   → Fusion 5/11 terminée\n",
      "   → Fusion 6/11 terminée\n",
      "   → Fusion 7/11 terminée\n",
      "   → Fusion 8/11 terminée\n",
      "   → Fusion 9/11 terminée\n",
      "   → Fusion 10/11 terminée\n",
      "   → Fusion 11/11 terminée\n",
      "\n",
      " DataFrame climatique fusionné créé : (10113, 14)\n",
      " Colonnes : ['longitude', 'latitude', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4']\n",
      "\n",
      " Aperçu :\n",
      "   longitude   latitude  log_precip_s1  log_precip_s2  log_precip_s3  \\\n",
      "0     -8.625  27.291667       3.082025       0.916291       0.832909   \n",
      "1     -8.625  27.375000       3.224341       0.916291       1.964311   \n",
      "2     -8.625  27.458333       3.655782       1.321756       1.996060   \n",
      "3     -8.625  27.541667       3.654403       1.267652       1.163151   \n",
      "4     -8.625  27.625000       2.014903       1.357123       1.996060   \n",
      "\n",
      "   log_precip_s4    tmax_s1    tmax_s2  tmax_s3    tmax_s4  \\\n",
      "0       8.040502  22.916667  32.625000   39.000  31.333333   \n",
      "1       8.127222  22.750000  31.250000   41.125  31.333333   \n",
      "2       8.197978  22.666667  29.250000   41.250  31.333333   \n",
      "3       6.103607  22.333333  29.000000   41.000  31.500000   \n",
      "4       8.313977  21.000000  30.333333   41.000  31.000000   \n",
      "\n",
      "   amplitude_thermique_s1  amplitude_thermique_s2  amplitude_thermique_s3  \\\n",
      "0               13.916667               14.000000                  14.125   \n",
      "1               13.583333               13.250000                  14.125   \n",
      "2               13.500000               14.000000                  14.250   \n",
      "3               13.416667               14.000000                  14.250   \n",
      "4               13.000000               14.166667                  14.375   \n",
      "\n",
      "   amplitude_thermique_s4  \n",
      "0               13.000000  \n",
      "1               13.000000  \n",
      "2               13.250000  \n",
      "3               14.000000  \n",
      "4               13.333333  \n"
     ]
    }
   ],
   "source": [
    "# Commencer avec le premier DataFrame\n",
    "df_climat_merged = all_dfs[0]\n",
    "\n",
    "# Fusionner tous les autres DataFrames sur longitude et latitude\n",
    "for i, df in enumerate(all_dfs[1:], 1):\n",
    "    df_climat_merged = pd.merge(df_climat_merged, df, on=['longitude', 'latitude'], how='inner')\n",
    "    print(f\"   → Fusion {i}/{len(all_dfs)-1} terminée\")\n",
    "\n",
    "print(f\"\\n DataFrame climatique fusionné créé : {df_climat_merged.shape}\")\n",
    "print(f\" Colonnes : {list(df_climat_merged.columns)}\")\n",
    "print(\"\\n Aperçu :\")\n",
    "print(df_climat_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ca2b1",
   "metadata": {},
   "source": [
    "## Merge final : Fire + Climat (approche optimisée avec pd.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed6077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordonnées feu avant arrondi : lon=9.489470, lat=31.492900\n",
      "Coordonnées feu après arrondi : lon=9.500000, lat=31.500000\n",
      "\n",
      "Dataset final après merge : (60463, 15)\n",
      "Valeurs manquantes dans les variables climatiques :\n",
      "log_precip_s1             44155\n",
      "log_precip_s2             44155\n",
      "log_precip_s3             44155\n",
      "log_precip_s4             44155\n",
      "tmax_s1                   44155\n",
      "tmax_s2                   44155\n",
      "tmax_s3                   44155\n",
      "tmax_s4                   44155\n",
      "amplitude_thermique_s1    44155\n",
      "amplitude_thermique_s2    44155\n",
      "amplitude_thermique_s3    44155\n",
      "amplitude_thermique_s4    44155\n",
      "dtype: int64\n",
      "\n",
      "Points avec données climatiques : 16308/60463 (27.0%)\n"
     ]
    }
   ],
   "source": [
    "# Arrondir les coordonnées à la résolution de la grille climatique\n",
    "resolution = 1/12  # 0.083333°\n",
    "\n",
    "# Fonction d'arrondissement améliorée\n",
    "def round_to_grid(value, resolution):\n",
    "    return np.round(value / resolution) * resolution\n",
    "\n",
    "# Créer des copies avec coordonnées arrondies POUR LES DEUX DATASETS\n",
    "df_fire_for_merge = df_fire_clean.copy()\n",
    "df_fire_for_merge['lon_rounded'] = round_to_grid(df_fire_clean['lon'], resolution)\n",
    "df_fire_for_merge['lat_rounded'] = round_to_grid(df_fire_clean['lat'], resolution)\n",
    "\n",
    "df_climat_for_merge = df_climat_merged.copy()\n",
    "df_climat_for_merge['longitude_rounded'] = round_to_grid(df_climat_merged['longitude'], resolution)\n",
    "df_climat_for_merge['latitude_rounded'] = round_to_grid(df_climat_merged['latitude'], resolution)\n",
    "\n",
    "print(f\"Coordonnées feu avant arrondi : lon={df_fire_clean['lon'].iloc[0]:.6f}, lat={df_fire_clean['lat'].iloc[0]:.6f}\")\n",
    "print(f\"Coordonnées feu après arrondi : lon={df_fire_for_merge['lon_rounded'].iloc[0]:.6f}, lat={df_fire_for_merge['lat_rounded'].iloc[0]:.6f}\")\n",
    "\n",
    "# Merge direct avec les colonnes arrondies\n",
    "df_final = pd.merge(\n",
    "    df_fire_for_merge,\n",
    "    df_climat_for_merge,\n",
    "    left_on=['lon_rounded', 'lat_rounded'],\n",
    "    right_on=['longitude_rounded', 'latitude_rounded'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Supprimer les colonnes temporaires\n",
    "cols_to_drop = ['lon_rounded', 'lat_rounded', 'longitude_rounded', 'latitude_rounded', 'longitude', 'latitude']\n",
    "df_final = df_final.drop(columns=[col for col in cols_to_drop if col in df_final.columns])\n",
    "\n",
    "print(f\"\\nDataset final après merge : {df_final.shape}\")\n",
    "print(f\"Valeurs manquantes dans les variables climatiques :\")\n",
    "climatic_cols = [col for col in df_final.columns if any(season in col for season in ['s1', 's2', 's3', 's4'])]\n",
    "print(df_final[climatic_cols].isnull().sum())\n",
    "\n",
    "# Vérifier le pourcentage de correspondances\n",
    "matches_count = df_final[climatic_cols[0]].notna().sum()\n",
    "total_points = len(df_final)\n",
    "print(f\"\\nPoints avec données climatiques : {matches_count}/{total_points} ({matches_count/total_points*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1b3ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anfel\\anaconda3\\envs\\dm\\lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land cover chargé : 307385 polygones\n",
      "CRS : EPSG:4326\n",
      "Colonnes : ['GRIDCODE', 'pays', 'log_area_sqm', 'lcc_code_encoded', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# Charger landcover avec les polygones originaux\n",
    "gdf_land = gpd.read_file(r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\landcover_final_ML.gpkg\")\n",
    "\n",
    "print(f\"Land cover chargé : {gdf_land.shape[0]} polygones\")\n",
    "print(f\"CRS : {gdf_land.crs}\")\n",
    "print(f\"Colonnes : {list(gdf_land.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2871f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame créé : (60463, 16)\n",
      "CRS : EPSG:4326\n",
      "Type de géométrie : ['Point']\n"
     ]
    }
   ],
   "source": [
    "# Convertir df_final en GeoDataFrame avec des points géométriques\n",
    "gdf_final = gpd.GeoDataFrame(\n",
    "    df_final,\n",
    "    geometry=gpd.points_from_xy(df_final['lon'], df_final['lat']),\n",
    "    crs=\"EPSG:4326\"  # WGS84 (système de coordonnées standard)\n",
    ")\n",
    "\n",
    "print(f\"\\nGeoDataFrame créé : {gdf_final.shape}\")\n",
    "print(f\"CRS : {gdf_final.crs}\")\n",
    "print(f\"Type de géométrie : {gdf_final.geometry.geom_type.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42191957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Les CRS sont déjà alignés : EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# Aligner les systèmes de coordonnées si nécessaire\n",
    "if gdf_final.crs != gdf_land.crs:\n",
    "    print(f\"\\nReprojection de gdf_land : {gdf_land.crs} → {gdf_final.crs}\")\n",
    "    gdf_land = gdf_land.to_crs(gdf_final.crs)\n",
    "else:\n",
    "    print(f\"\\nLes CRS sont déjà alignés : {gdf_final.crs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824d5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Points (Fire + Climat):\n",
      "   Min Lon: -8.1280, Max Lon: 11.1119\n",
      "   Min Lat: 19.5932, Max Lat: 37.3332\n",
      "\n",
      "  Polygones (Land Cover):\n",
      "   Min Lon: -8.6722, Max Lon: 11.9681\n",
      "   Min Lat: 18.9660, Max Lat: 37.5439\n",
      " Les zones SE CHEVAUCHENT \n",
      "\n",
      " ÉCHANTILLON DE POINTS (5 premiers)\n",
      "        lon       lat                   geometry\n",
      "0   9.48947  31.49290    POINT (9.48947 31.4929)\n",
      "1   9.49053  31.49524   POINT (9.49053 31.49524)\n",
      "2   9.49368  31.49449   POINT (9.49368 31.49449)\n",
      "3   9.49154  31.49420    POINT (9.49154 31.4942)\n",
      "4  10.09115  36.93407  POINT (10.09115 36.93407)\n",
      "\n",
      " ÉCHANTILLON DE POLYGONES (5 premiers)\n",
      "   GRIDCODE                                           geometry\n",
      "0       210  POLYGON ((6.41528 37.08696, 6.43103 37.0855, 6...\n",
      "1       210  POLYGON ((7.37137 37.08194, 7.3709 37.08717, 7...\n",
      "2        50  POLYGON ((6.12361 36.68472, 6.12361 36.69306, ...\n",
      "3       130  POLYGON ((6.44583 37.07917, 6.44583 37.08194, ...\n",
      "4        50  POLYGON ((6.40694 37.08194, 6.40694 37.08472, ...\n",
      "\n",
      " VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\n",
      "   Min Lon: -8.6722, Max Lon: 11.9681\n",
      "   Min Lat: 18.9660, Max Lat: 37.5439\n",
      " Les zones SE CHEVAUCHENT \n",
      "\n",
      " ÉCHANTILLON DE POINTS (5 premiers)\n",
      "        lon       lat                   geometry\n",
      "0   9.48947  31.49290    POINT (9.48947 31.4929)\n",
      "1   9.49053  31.49524   POINT (9.49053 31.49524)\n",
      "2   9.49368  31.49449   POINT (9.49368 31.49449)\n",
      "3   9.49154  31.49420    POINT (9.49154 31.4942)\n",
      "4  10.09115  36.93407  POINT (10.09115 36.93407)\n",
      "\n",
      " ÉCHANTILLON DE POLYGONES (5 premiers)\n",
      "   GRIDCODE                                           geometry\n",
      "0       210  POLYGON ((6.41528 37.08696, 6.43103 37.0855, 6...\n",
      "1       210  POLYGON ((7.37137 37.08194, 7.3709 37.08717, 7...\n",
      "2        50  POLYGON ((6.12361 36.68472, 6.12361 36.69306, ...\n",
      "3       130  POLYGON ((6.44583 37.07917, 6.44583 37.08194, ...\n",
      "4        50  POLYGON ((6.40694 37.08194, 6.40694 37.08472, ...\n",
      "\n",
      " VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\n",
      "   Points invalides : 0/60463\n",
      "   Polygones invalides : 0/307385\n",
      "    Toutes les géométries sont valides\n",
      "   Points invalides : 0/60463\n",
      "   Polygones invalides : 0/307385\n",
      "    Toutes les géométries sont valides\n"
     ]
    }
   ],
   "source": [
    "# 1. Vérifier les bounding boxes\n",
    "print(\"\\n Points (Fire + Climat):\")\n",
    "points_bounds = gdf_final.total_bounds\n",
    "print(f\"   Min Lon: {points_bounds[0]:.4f}, Max Lon: {points_bounds[2]:.4f}\")\n",
    "print(f\"   Min Lat: {points_bounds[1]:.4f}, Max Lat: {points_bounds[3]:.4f}\")\n",
    "\n",
    "print(\"\\n  Polygones (Land Cover):\")\n",
    "land_bounds = gdf_land.total_bounds\n",
    "print(f\"   Min Lon: {land_bounds[0]:.4f}, Max Lon: {land_bounds[2]:.4f}\")\n",
    "print(f\"   Min Lat: {land_bounds[1]:.4f}, Max Lat: {land_bounds[3]:.4f}\")\n",
    "\n",
    "# 2. Vérifier si les zones se chevauchent\n",
    "lon_overlap = not (points_bounds[2] < land_bounds[0] or points_bounds[0] > land_bounds[2])\n",
    "lat_overlap = not (points_bounds[3] < land_bounds[1] or points_bounds[1] > land_bounds[3])\n",
    "\n",
    "if lon_overlap and lat_overlap:\n",
    "    print(\" Les zones SE CHEVAUCHENT \")\n",
    "else:\n",
    "    print(\" Les zones NE SE CHEVAUCHENT PAS \")\n",
    "    if not lon_overlap:\n",
    "        print(\" Pas de chevauchement en LONGITUDE\")\n",
    "    if not lat_overlap:\n",
    "        print(\"Pas de chevauchement en LATITUDE\")\n",
    "\n",
    "# 3. Échantillon de points\n",
    "print(\"\\n ÉCHANTILLON DE POINTS (5 premiers)\")\n",
    "print(gdf_final[['lon', 'lat', 'geometry']].head())\n",
    "\n",
    "# 4. Échantillon de polygones\n",
    "print(\"\\n ÉCHANTILLON DE POLYGONES (5 premiers)\")\n",
    "print(gdf_land[['GRIDCODE', 'geometry']].head())\n",
    "\n",
    "# 5. Vérifier les géométries invalides\n",
    "print(\"\\n VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\")\n",
    "invalid_points = (~gdf_final.is_valid).sum()\n",
    "invalid_polys = (~gdf_land.is_valid).sum()\n",
    "print(f\"   Points invalides : {invalid_points}/{len(gdf_final)}\")\n",
    "print(f\"   Polygones invalides : {invalid_polys}/{len(gdf_land)}\")\n",
    "\n",
    "if invalid_points > 0 or invalid_polys > 0:\n",
    "    print(\"    Il y a des géométries invalides !\")\n",
    "else:\n",
    "    print(\"    Toutes les géométries sont valides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a8040d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfel\\AppData\\Local\\Temp\\ipykernel_8408\\1355669867.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_final_buffered['geometry'] = gdf_final_buffered.geometry.buffer(BUFFER_SIZE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RÉSULTATS :\n",
      "   • Total de lignes après join : 71,724\n",
      "   • Points uniques traités : 60,463\n",
      "   • Lignes avec correspondance : 31,934\n",
      "   • Points uniques ayant trouvé un match : 20,673\n",
      "   • Taux de couverture : 34.2%\n",
      "\n",
      "  11,261 lignes dupliquées détectées\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 0.005  # 500 mètres\n",
    "\n",
    "# Créer une copie avec buffer\n",
    "gdf_final_buffered = gdf_final.copy()\n",
    "gdf_final_buffered['geometry'] = gdf_final_buffered.geometry.buffer(BUFFER_SIZE)\n",
    "\n",
    "# Spatial join sur tout le dataset\n",
    "gdf_full = gpd.sjoin(\n",
    "    gdf_final_buffered,\n",
    "    gdf_land[['geometry', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']],\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "\n",
    "# Statistiques\n",
    "total_rows = len(gdf_full)\n",
    "unique_points = gdf_full.index.nunique()\n",
    "matches = gdf_full['GRIDCODE'].notna().sum()\n",
    "unique_matches = gdf_full[gdf_full['GRIDCODE'].notna()].index.nunique()\n",
    "\n",
    "\n",
    "print(f\"\\n RÉSULTATS :\")\n",
    "print(f\"   • Total de lignes après join : {total_rows:,}\")\n",
    "print(f\"   • Points uniques traités : {unique_points:,}\")\n",
    "print(f\"   • Lignes avec correspondance : {matches:,}\")\n",
    "print(f\"   • Points uniques ayant trouvé un match : {unique_matches:,}\")\n",
    "print(f\"   • Taux de couverture : {(unique_matches/unique_points)*100:.1f}%\")\n",
    "\n",
    "# Vérifier les duplications\n",
    "duplications = total_rows - unique_points\n",
    "if duplications > 0:\n",
    "    print(f\"\\n  {duplications:,} lignes dupliquées détectées\")\n",
    "else:\n",
    "    print(f\"\\n Aucune duplication \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e4e4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avant dé-duplication : 71,724 lignes\n",
      "   Après dé-duplication : 60,463 lignes\n",
      "\n",
      "DATASET FINAL :\n",
      "   Shape : (60463, 18)\n",
      "   Colonnes : ['lon', 'lat', 'fire', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']\n",
      "\n",
      " COUVERTURE LAND COVER :\n",
      "   Points avec land cover : 20,673/60,463 (34.2%)\n",
      "   Points sans land cover : 39,790 (65.8%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les duplications (garder la première correspondance pour chaque point)\n",
    "if len(gdf_full) > len(gdf_final):\n",
    "    print(f\"   Avant dé-duplication : {len(gdf_full):,} lignes\")\n",
    "    gdf_full = gdf_full[~gdf_full.index.duplicated(keep='first')]\n",
    "    print(f\"   Après dé-duplication : {len(gdf_full):,} lignes\")\n",
    "    removed = len(gdf_full) - len(gdf_final)\n",
    "    if removed != 0:\n",
    "        print(f\"Duplications supprimées\")\n",
    "else:\n",
    "    print(\" Aucune duplication à nettoyer\")\n",
    "\n",
    "# Nettoyage final : supprimer les colonnes inutiles\n",
    "df_full = gdf_full.drop(columns=['geometry', 'index_right'], errors='ignore')\n",
    "df_full = pd.DataFrame(df_full)\n",
    "\n",
    "print(f\"\\nDATASET FINAL :\")\n",
    "print(f\"   Shape : {df_full.shape}\")\n",
    "print(f\"   Colonnes : {list(df_full.columns)}\")\n",
    "\n",
    "# Statistiques finales\n",
    "land_cover_matches = df_full['GRIDCODE'].notna().sum()\n",
    "total_points = len(df_full)\n",
    "print(f\"\\n COUVERTURE LAND COVER :\")\n",
    "print(f\"   Points avec land cover : {land_cover_matches:,}/{total_points:,} ({(land_cover_matches/total_points)*100:.1f}%)\")\n",
    "print(f\"   Points sans land cover : {total_points - land_cover_matches:,} ({((total_points - land_cover_matches)/total_points)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f917d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfel\\AppData\\Local\\Temp\\ipykernel_8408\\2233567768.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_land_centroids['geometry'] = gdf_land_centroids.geometry.centroid\n",
      "c:\\Users\\anfel\\anaconda3\\envs\\dm\\lib\\site-packages\\geopandas\\array.py:408: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\anfel\\anaconda3\\envs\\dm\\lib\\site-packages\\geopandas\\array.py:408: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RÉSULTATS AVEC CENTROÏDES :\n",
      "   • Dataset shape : (60463, 20)\n",
      "   • Points avec land cover : 60,463 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Créer les centroïdes des polygones land cover\n",
    "gdf_land_centroids = gdf_land.copy()\n",
    "gdf_land_centroids['geometry'] = gdf_land_centroids.geometry.centroid\n",
    "\n",
    "# sjoin_nearest trouve automatiquement le point le plus proche\n",
    "gdf_full_centroids = gpd.sjoin_nearest(\n",
    "    gdf_final,\n",
    "    gdf_land_centroids[['geometry', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']],\n",
    "    how='left',\n",
    "    distance_col='distance_to_centroid_deg'\n",
    ")\n",
    "\n",
    "# Convertir la distance en mètres\n",
    "gdf_full_centroids['distance_to_centroid_m'] = gdf_full_centroids['distance_to_centroid_deg'] * 111000\n",
    "\n",
    "# Nettoyer les colonnes inutiles\n",
    "df_full_centroids = gdf_full_centroids.drop(columns=['geometry', 'index_right'], errors='ignore')\n",
    "df_full_centroids = pd.DataFrame(df_full_centroids)\n",
    "\n",
    "print(f\"\\n RÉSULTATS AVEC CENTROÏDES :\")\n",
    "print(f\"   • Dataset shape : {df_full_centroids.shape}\")\n",
    "print(f\"   • Points avec land cover : {df_full_centroids['GRIDCODE'].notna().sum():,} ({(df_full_centroids['GRIDCODE'].notna().sum()/len(df_full_centroids))*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "210086b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le dataset final en format Parquet\n",
    "output_path = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\local_dataset\\dataset\\buffer.parquet\"\n",
    "\n",
    "df_full_centroids.to_parquet(output_path, engine='fastparquet', compression='snappy')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
