{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08503215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lon       lat  fire\n",
      "0   9.48947  31.49290     1\n",
      "1   9.49053  31.49524     1\n",
      "2   9.49368  31.49449     1\n",
      "3   9.49154  31.49420     1\n",
      "4  10.09115  36.93407     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "#path_fire = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\algeria_tunisia.csv\"\n",
    "#df_fire = pd.read_csv(path_fire)\n",
    "df_fire_clean = pd.read_parquet(r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\local_dataset\\dataset\\fire_full.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "#df_fire_clean = df_fire[['longitude', 'latitude', 'type']].copy()\n",
    "\n",
    "print(df_fire_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7102209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans df_fire_clean : 56864\n",
      "Shape du dataset : (56864, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de lignes dans df_fire_clean : {len(df_fire_clean)}\")\n",
    "print(f\"Shape du dataset : {df_fire_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcf82e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_fire_clean[\\'fire\\'] = np.where(df_fire_clean[\\'type\\'] == 0, 1, 0)\\n\\nprint(f\"df_fire_clean avec colonne \\'fire\\' créé : {df_fire_clean.shape}\")\\nprint(df_fire_clean.head())'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer l'attribut binaire 'fire'\n",
    "# Si 'type' == 0, 'fire' = 1, sinon 'fire' = 0\n",
    "\"\"\"df_fire_clean['fire'] = np.where(df_fire_clean['type'] == 0, 1, 0)\n",
    "\n",
    "print(f\"df_fire_clean avec colonne 'fire' créé : {df_fire_clean.shape}\")\n",
    "print(df_fire_clean.head())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad3645",
   "metadata": {},
   "source": [
    "## Chargement et agrégation des données climatiques par saisons + Conversion en DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb821f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV climat chargé : (148292, 6)\n",
      "\n",
      "Traitement de la variable : log_precip\n",
      "  → log_precip_s1 agrégé (20301 points)\n",
      "  → log_precip_s2 agrégé (20231 points)\n",
      "  → log_precip_s3 agrégé (16200 points)\n",
      "  → log_precip_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : tmax\n",
      "\n",
      "Traitement de la variable : log_precip\n",
      "  → log_precip_s1 agrégé (20301 points)\n",
      "  → log_precip_s2 agrégé (20231 points)\n",
      "  → log_precip_s3 agrégé (16200 points)\n",
      "  → log_precip_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : tmax\n",
      "  → tmax_s1 agrégé (20301 points)\n",
      "  → tmax_s2 agrégé (20231 points)\n",
      "  → tmax_s3 agrégé (16200 points)\n",
      "  → tmax_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : amplitude_thermique\n",
      "  → amplitude_thermique_s1 agrégé (20301 points)\n",
      "  → amplitude_thermique_s2 agrégé (20231 points)\n",
      "  → amplitude_thermique_s3 agrégé (16200 points)\n",
      "  → amplitude_thermique_s4 agrégé (21540 points)\n",
      "\n",
      " Toutes les variables saisonnières climatiques ont été générées.\n",
      "  → tmax_s1 agrégé (20301 points)\n",
      "  → tmax_s2 agrégé (20231 points)\n",
      "  → tmax_s3 agrégé (16200 points)\n",
      "  → tmax_s4 agrégé (21540 points)\n",
      "\n",
      "Traitement de la variable : amplitude_thermique\n",
      "  → amplitude_thermique_s1 agrégé (20301 points)\n",
      "  → amplitude_thermique_s2 agrégé (20231 points)\n",
      "  → amplitude_thermique_s3 agrégé (16200 points)\n",
      "  → amplitude_thermique_s4 agrégé (21540 points)\n",
      "\n",
      " Toutes les variables saisonnières climatiques ont été générées.\n"
     ]
    }
   ],
   "source": [
    "climate_csv_path = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\climat_clean.csv\"\n",
    "climate_df = pd.read_csv(climate_csv_path)\n",
    "\n",
    "print(\"CSV climat chargé :\", climate_df.shape)\n",
    "\n",
    "# Convertir la date\n",
    "climate_df[\"time\"] = pd.to_datetime(climate_df[\"time\"])\n",
    "\n",
    "seasons_indices = {\n",
    "    's1': [12, 1, 2],   # Hiver (Dec-Jan-Feb)\n",
    "    's2': [3, 4, 5],    # Printemps\n",
    "    's3': [6, 7, 8],    # Été\n",
    "    's4': [9, 10, 11]   # Automne\n",
    "}\n",
    "\n",
    "# Ajouter colonne \"season\"\n",
    "\n",
    "def get_season(date):\n",
    "    m = date.month\n",
    "    if m in [12, 1, 2]:\n",
    "        return \"s1\"\n",
    "    if m in [3, 4, 5]:\n",
    "        return \"s2\"\n",
    "    if m in [6, 7, 8]:\n",
    "        return \"s3\"\n",
    "    return \"s4\"\n",
    "\n",
    "climate_df[\"season\"] = climate_df[\"time\"].apply(get_season)\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "variables = [\"log_precip\",  \"tmax\",\"amplitude_thermique\"]\n",
    "\n",
    "for var in variables:\n",
    "    print(f\"\\nTraitement de la variable : {var}\")\n",
    "\n",
    "    for season in [\"s1\", \"s2\", \"s3\", \"s4\"]:\n",
    "        \n",
    "        df_season = climate_df[climate_df[\"season\"] == season]\n",
    "\n",
    "        if df_season.empty:\n",
    "            print(f\"   Saison {season} vide pour {var}\")\n",
    "            continue\n",
    "\n",
    "        # Agrégation\n",
    "        if var == \"log_precip\":\n",
    "            df_agg = (\n",
    "                df_season.groupby([\"longitude\", \"latitude\"])[var]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .rename(columns={var: f\"{var}_{season}\"})\n",
    "            )\n",
    "        else:\n",
    "            df_agg = (\n",
    "                df_season.groupby([\"longitude\", \"latitude\"])[var]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={var: f\"{var}_{season}\"})\n",
    "            )\n",
    "\n",
    "        all_dfs.append(df_agg)\n",
    "\n",
    "        print(f\"  → {var}_{season} agrégé ({df_agg.shape[0]} points)\")\n",
    "\n",
    "\n",
    "print(\"\\n Toutes les variables saisonnières climatiques ont été générées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94a4d3",
   "metadata": {},
   "source": [
    "## Fusion des DataFrames climatiques en un seul DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9043fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → Fusion 1/11 terminée\n",
      "   → Fusion 2/11 terminée\n",
      "   → Fusion 3/11 terminée\n",
      "   → Fusion 4/11 terminée\n",
      "   → Fusion 5/11 terminée\n",
      "   → Fusion 6/11 terminée\n",
      "   → Fusion 7/11 terminée\n",
      "   → Fusion 8/11 terminée\n",
      "   → Fusion 9/11 terminée\n",
      "   → Fusion 10/11 terminée\n",
      "   → Fusion 11/11 terminée\n",
      "\n",
      " DataFrame climatique fusionné créé : (10113, 14)\n",
      " Colonnes : ['longitude', 'latitude', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4']\n",
      "\n",
      " Aperçu :\n",
      "   longitude   latitude  log_precip_s1  log_precip_s2  log_precip_s3  \\\n",
      "0     -8.625  27.291667       3.082025       0.916291       0.832909   \n",
      "1     -8.625  27.375000       3.224341       0.916291       1.964311   \n",
      "2     -8.625  27.458333       3.655782       1.321756       1.996060   \n",
      "3     -8.625  27.541667       3.654403       1.267652       1.163151   \n",
      "4     -8.625  27.625000       2.014903       1.357123       1.996060   \n",
      "\n",
      "   log_precip_s4    tmax_s1    tmax_s2  tmax_s3    tmax_s4  \\\n",
      "0       8.040502  22.916667  32.625000   39.000  31.333333   \n",
      "1       8.127222  22.750000  31.250000   41.125  31.333333   \n",
      "2       8.197978  22.666667  29.250000   41.250  31.333333   \n",
      "3       6.103607  22.333333  29.000000   41.000  31.500000   \n",
      "4       8.313977  21.000000  30.333333   41.000  31.000000   \n",
      "\n",
      "   amplitude_thermique_s1  amplitude_thermique_s2  amplitude_thermique_s3  \\\n",
      "0               13.916667               14.000000                  14.125   \n",
      "1               13.583333               13.250000                  14.125   \n",
      "2               13.500000               14.000000                  14.250   \n",
      "3               13.416667               14.000000                  14.250   \n",
      "4               13.000000               14.166667                  14.375   \n",
      "\n",
      "   amplitude_thermique_s4  \n",
      "0               13.000000  \n",
      "1               13.000000  \n",
      "2               13.250000  \n",
      "3               14.000000  \n",
      "4               13.333333  \n"
     ]
    }
   ],
   "source": [
    "# Commencer avec le premier DataFrame\n",
    "df_climat_merged = all_dfs[0]\n",
    "\n",
    "# Fusionner tous les autres DataFrames sur longitude et latitude\n",
    "for i, df in enumerate(all_dfs[1:], 1):\n",
    "    df_climat_merged = pd.merge(df_climat_merged, df, on=['longitude', 'latitude'], how='inner')\n",
    "    print(f\"   → Fusion {i}/{len(all_dfs)-1} terminée\")\n",
    "\n",
    "print(f\"\\n DataFrame climatique fusionné créé : {df_climat_merged.shape}\")\n",
    "print(f\" Colonnes : {list(df_climat_merged.columns)}\")\n",
    "print(\"\\n Aperçu :\")\n",
    "print(df_climat_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ca2b1",
   "metadata": {},
   "source": [
    "## Merge final : Fire + Climat (approche optimisée avec pd.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed6077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire coords shape: (56864, 2)\n",
      "Climat coords shape: (10113, 2)\n",
      "Construction du KDTree...\n",
      "KDTree construit.\n",
      "Recherche des plus proches voisins...\n",
      "Plus proches voisins trouvés.\n",
      "Distance moyenne: 0.307297°\n",
      "Distance max: 2.171342°\n",
      "Distance min: 0.000970°\n",
      "\n",
      "Dataset final après merge : (56864, 15)\n",
      "Colonnes : ['lon', 'lat', 'fire', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4']\n",
      "\n",
      "Valeurs manquantes dans les variables climatiques :\n",
      "log_precip_s1             0\n",
      "log_precip_s2             0\n",
      "log_precip_s3             0\n",
      "log_precip_s4             0\n",
      "tmax_s1                   0\n",
      "tmax_s2                   0\n",
      "tmax_s3                   0\n",
      "tmax_s4                   0\n",
      "amplitude_thermique_s1    0\n",
      "amplitude_thermique_s2    0\n",
      "amplitude_thermique_s3    0\n",
      "amplitude_thermique_s4    0\n",
      "dtype: int64\n",
      "\n",
      "Points avec données climatiques : 56864/56864 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Merge Fire + Climat avec KDTree\n",
    "\n",
    "# Extraire les coordonnées des points de feu\n",
    "fire_coords = df_fire_clean[[\"lon\", \"lat\"]].to_numpy()\n",
    "print(f\"Fire coords shape: {fire_coords.shape}\")\n",
    "\n",
    "# Extraire les coordonnées de la grille climatique\n",
    "climat_coords = df_climat_merged[[\"longitude\", \"latitude\"]].to_numpy()\n",
    "print(f\"Climat coords shape: {climat_coords.shape}\")\n",
    "\n",
    "# Construire le KDTree sur la grille climatique\n",
    "print(\"Construction du KDTree...\")\n",
    "tree = cKDTree(climat_coords)\n",
    "print(\"KDTree construit.\")\n",
    "\n",
    "# Trouver le point de grille le plus proche pour chaque point de feu\n",
    "print(\"Recherche des plus proches voisins...\")\n",
    "dist, idx = tree.query(fire_coords, k=1)\n",
    "print(\"Plus proches voisins trouvés.\")\n",
    "\n",
    "print(f\"Distance moyenne: {dist.mean():.6f}°\")\n",
    "print(f\"Distance max: {dist.max():.6f}°\")\n",
    "print(f\"Distance min: {dist.min():.6f}°\")\n",
    "\n",
    "# Fusionner les données climatiques avec les données de feu\n",
    "df_final = pd.concat([\n",
    "    df_fire_clean.reset_index(drop=True),\n",
    "    df_climat_merged.drop(columns=[\"longitude\", \"latitude\"]).iloc[idx.flatten()].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nDataset final après merge : {df_final.shape}\")\n",
    "print(f\"Colonnes : {list(df_final.columns)}\")\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "climatic_cols = [col for col in df_final.columns if any(season in col for season in ['s1', 's2', 's3', 's4'])]\n",
    "print(f\"\\nValeurs manquantes dans les variables climatiques :\")\n",
    "print(df_final[climatic_cols].isnull().sum())\n",
    "\n",
    "# Vérifier le pourcentage de correspondances\n",
    "matches_count = df_final[climatic_cols[0]].notna().sum()\n",
    "total_points = len(df_final)\n",
    "print(f\"\\nPoints avec données climatiques : {matches_count}/{total_points} ({matches_count/total_points*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1b3ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anfel\\anaconda3\\envs\\dm\\lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land cover chargé : 307385 polygones\n",
      "CRS : EPSG:4326\n",
      "Colonnes : ['GRIDCODE', 'pays', 'log_area_sqm', 'lcc_code_encoded', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# Charger landcover avec les polygones originaux\n",
    "gdf_land = gpd.read_file(r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\data\\landcover_final_ML.gpkg\")\n",
    "\n",
    "print(f\"Land cover chargé : {gdf_land.shape[0]} polygones\")\n",
    "print(f\"CRS : {gdf_land.crs}\")\n",
    "print(f\"Colonnes : {list(gdf_land.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2871f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoDataFrame créé : (56864, 16)\n",
      "CRS : EPSG:4326\n",
      "Type de géométrie : ['Point']\n"
     ]
    }
   ],
   "source": [
    "# Convertir df_final en GeoDataFrame avec des points géométriques\n",
    "gdf_final = gpd.GeoDataFrame(\n",
    "    df_final,\n",
    "    geometry=gpd.points_from_xy(df_final['lon'], df_final['lat']),\n",
    "    crs=\"EPSG:4326\"  # WGS84 (système de coordonnées standard)\n",
    ")\n",
    "\n",
    "print(f\"\\nGeoDataFrame créé : {gdf_final.shape}\")\n",
    "print(f\"CRS : {gdf_final.crs}\")\n",
    "print(f\"Type de géométrie : {gdf_final.geometry.geom_type.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42191957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Les CRS sont déjà alignés : EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# Aligner les systèmes de coordonnées si nécessaire\n",
    "if gdf_final.crs != gdf_land.crs:\n",
    "    print(f\"\\nReprojection de gdf_land : {gdf_land.crs} → {gdf_final.crs}\")\n",
    "    gdf_land = gdf_land.to_crs(gdf_final.crs)\n",
    "else:\n",
    "    print(f\"\\nLes CRS sont déjà alignés : {gdf_final.crs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824d5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Points (Fire + Climat):\n",
      "   Min Lon: -8.1280, Max Lon: 11.1119\n",
      "   Min Lat: 19.5932, Max Lat: 37.3332\n",
      "\n",
      "  Polygones (Land Cover):\n",
      "   Min Lon: -8.6722, Max Lon: 11.9681\n",
      "   Min Lat: 18.9660, Max Lat: 37.5439\n",
      " Les zones SE CHEVAUCHENT \n",
      "\n",
      " ÉCHANTILLON DE POINTS (5 premiers)\n",
      "        lon       lat                   geometry\n",
      "0   9.48947  31.49290    POINT (9.48947 31.4929)\n",
      "1   9.49053  31.49524   POINT (9.49053 31.49524)\n",
      "2   9.49368  31.49449   POINT (9.49368 31.49449)\n",
      "3   9.49154  31.49420    POINT (9.49154 31.4942)\n",
      "4  10.09115  36.93407  POINT (10.09115 36.93407)\n",
      "\n",
      " ÉCHANTILLON DE POLYGONES (5 premiers)\n",
      "   GRIDCODE                                           geometry\n",
      "0       210  POLYGON ((6.41528 37.08696, 6.43103 37.0855, 6...\n",
      "1       210  POLYGON ((7.37137 37.08194, 7.3709 37.08717, 7...\n",
      "2        50  POLYGON ((6.12361 36.68472, 6.12361 36.69306, ...\n",
      "3       130  POLYGON ((6.44583 37.07917, 6.44583 37.08194, ...\n",
      "4        50  POLYGON ((6.40694 37.08194, 6.40694 37.08472, ...\n",
      "\n",
      " VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\n",
      "   Min Lon: -8.6722, Max Lon: 11.9681\n",
      "   Min Lat: 18.9660, Max Lat: 37.5439\n",
      " Les zones SE CHEVAUCHENT \n",
      "\n",
      " ÉCHANTILLON DE POINTS (5 premiers)\n",
      "        lon       lat                   geometry\n",
      "0   9.48947  31.49290    POINT (9.48947 31.4929)\n",
      "1   9.49053  31.49524   POINT (9.49053 31.49524)\n",
      "2   9.49368  31.49449   POINT (9.49368 31.49449)\n",
      "3   9.49154  31.49420    POINT (9.49154 31.4942)\n",
      "4  10.09115  36.93407  POINT (10.09115 36.93407)\n",
      "\n",
      " ÉCHANTILLON DE POLYGONES (5 premiers)\n",
      "   GRIDCODE                                           geometry\n",
      "0       210  POLYGON ((6.41528 37.08696, 6.43103 37.0855, 6...\n",
      "1       210  POLYGON ((7.37137 37.08194, 7.3709 37.08717, 7...\n",
      "2        50  POLYGON ((6.12361 36.68472, 6.12361 36.69306, ...\n",
      "3       130  POLYGON ((6.44583 37.07917, 6.44583 37.08194, ...\n",
      "4        50  POLYGON ((6.40694 37.08194, 6.40694 37.08472, ...\n",
      "\n",
      " VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\n",
      "   Points invalides : 0/56864\n",
      "   Polygones invalides : 0/307385\n",
      "    Toutes les géométries sont valides\n",
      "   Points invalides : 0/56864\n",
      "   Polygones invalides : 0/307385\n",
      "    Toutes les géométries sont valides\n"
     ]
    }
   ],
   "source": [
    "# 1. Vérifier les bounding boxes\n",
    "print(\"\\n Points (Fire + Climat):\")\n",
    "points_bounds = gdf_final.total_bounds\n",
    "print(f\"   Min Lon: {points_bounds[0]:.4f}, Max Lon: {points_bounds[2]:.4f}\")\n",
    "print(f\"   Min Lat: {points_bounds[1]:.4f}, Max Lat: {points_bounds[3]:.4f}\")\n",
    "\n",
    "print(\"\\n  Polygones (Land Cover):\")\n",
    "land_bounds = gdf_land.total_bounds\n",
    "print(f\"   Min Lon: {land_bounds[0]:.4f}, Max Lon: {land_bounds[2]:.4f}\")\n",
    "print(f\"   Min Lat: {land_bounds[1]:.4f}, Max Lat: {land_bounds[3]:.4f}\")\n",
    "\n",
    "# 2. Vérifier si les zones se chevauchent\n",
    "lon_overlap = not (points_bounds[2] < land_bounds[0] or points_bounds[0] > land_bounds[2])\n",
    "lat_overlap = not (points_bounds[3] < land_bounds[1] or points_bounds[1] > land_bounds[3])\n",
    "\n",
    "if lon_overlap and lat_overlap:\n",
    "    print(\" Les zones SE CHEVAUCHENT \")\n",
    "else:\n",
    "    print(\" Les zones NE SE CHEVAUCHENT PAS \")\n",
    "    if not lon_overlap:\n",
    "        print(\" Pas de chevauchement en LONGITUDE\")\n",
    "    if not lat_overlap:\n",
    "        print(\"Pas de chevauchement en LATITUDE\")\n",
    "\n",
    "# 3. Échantillon de points\n",
    "print(\"\\n ÉCHANTILLON DE POINTS (5 premiers)\")\n",
    "print(gdf_final[['lon', 'lat', 'geometry']].head())\n",
    "\n",
    "# 4. Échantillon de polygones\n",
    "print(\"\\n ÉCHANTILLON DE POLYGONES (5 premiers)\")\n",
    "print(gdf_land[['GRIDCODE', 'geometry']].head())\n",
    "\n",
    "# 5. Vérifier les géométries invalides\n",
    "print(\"\\n VÉRIFICATION DE LA VALIDITÉ DES GÉOMÉTRIES\")\n",
    "invalid_points = (~gdf_final.is_valid).sum()\n",
    "invalid_polys = (~gdf_land.is_valid).sum()\n",
    "print(f\"   Points invalides : {invalid_points}/{len(gdf_final)}\")\n",
    "print(f\"   Polygones invalides : {invalid_polys}/{len(gdf_land)}\")\n",
    "\n",
    "if invalid_points > 0 or invalid_polys > 0:\n",
    "    print(\"    Il y a des géométries invalides !\")\n",
    "else:\n",
    "    print(\"    Toutes les géométries sont valides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8040d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfel\\AppData\\Local\\Temp\\ipykernel_9156\\1355669867.py:5: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_final_buffered['geometry'] = gdf_final_buffered.geometry.buffer(BUFFER_SIZE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RÉSULTATS :\n",
      "   • Total de lignes après join : 66,426\n",
      "   • Points uniques traités : 56,864\n",
      "   • Lignes avec correspondance : 28,215\n",
      "   • Points uniques ayant trouvé un match : 18,653\n",
      "   • Taux de couverture : 32.8%\n",
      "\n",
      "  9,562 lignes dupliquées détectées\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 0.005  # 500 mètres\n",
    "\n",
    "# Créer une copie avec buffer\n",
    "gdf_final_buffered = gdf_final.copy()\n",
    "gdf_final_buffered['geometry'] = gdf_final_buffered.geometry.buffer(BUFFER_SIZE)\n",
    "\n",
    "# Spatial join sur tout le dataset\n",
    "gdf_full = gpd.sjoin(\n",
    "    gdf_final_buffered,\n",
    "    gdf_land[['geometry', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']],\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "\n",
    "# Statistiques\n",
    "total_rows = len(gdf_full)\n",
    "unique_points = gdf_full.index.nunique()\n",
    "matches = gdf_full['GRIDCODE'].notna().sum()\n",
    "unique_matches = gdf_full[gdf_full['GRIDCODE'].notna()].index.nunique()\n",
    "\n",
    "\n",
    "print(f\"\\n RÉSULTATS :\")\n",
    "print(f\"   • Total de lignes après join : {total_rows:,}\")\n",
    "print(f\"   • Points uniques traités : {unique_points:,}\")\n",
    "print(f\"   • Lignes avec correspondance : {matches:,}\")\n",
    "print(f\"   • Points uniques ayant trouvé un match : {unique_matches:,}\")\n",
    "print(f\"   • Taux de couverture : {(unique_matches/unique_points)*100:.1f}%\")\n",
    "\n",
    "# Vérifier les duplications\n",
    "duplications = total_rows - unique_points\n",
    "if duplications > 0:\n",
    "    print(f\"\\n  {duplications:,} lignes dupliquées détectées\")\n",
    "else:\n",
    "    print(f\"\\n Aucune duplication \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4e4a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avant dé-duplication : 66,426 lignes\n",
      "   Après dé-duplication : 56,864 lignes\n",
      "\n",
      "DATASET FINAL :\n",
      "   Shape : (56864, 18)\n",
      "   Colonnes : ['lon', 'lat', 'fire', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']\n",
      "\n",
      " COUVERTURE LAND COVER :\n",
      "   Points avec land cover : 18,653/56,864 (32.8%)\n",
      "   Points sans land cover : 38,211 (67.2%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les duplications (garder la première correspondance pour chaque point)\n",
    "if len(gdf_full) > len(gdf_final):\n",
    "    print(f\"   Avant dé-duplication : {len(gdf_full):,} lignes\")\n",
    "    gdf_full = gdf_full[~gdf_full.index.duplicated(keep='first')]\n",
    "    print(f\"   Après dé-duplication : {len(gdf_full):,} lignes\")\n",
    "    removed = len(gdf_full) - len(gdf_final)\n",
    "    if removed != 0:\n",
    "        print(f\"Duplications supprimées\")\n",
    "else:\n",
    "    print(\" Aucune duplication à nettoyer\")\n",
    "\n",
    "# Nettoyage final : supprimer les colonnes inutiles\n",
    "df_full = gdf_full.drop(columns=['geometry', 'index_right'], errors='ignore')\n",
    "df_full = pd.DataFrame(df_full)\n",
    "\n",
    "print(f\"\\nDATASET FINAL :\")\n",
    "print(f\"   Shape : {df_full.shape}\")\n",
    "print(f\"   Colonnes : {list(df_full.columns)}\")\n",
    "\n",
    "# Statistiques finales\n",
    "land_cover_matches = df_full['GRIDCODE'].notna().sum()\n",
    "total_points = len(df_full)\n",
    "print(f\"\\n COUVERTURE LAND COVER :\")\n",
    "print(f\"   Points avec land cover : {land_cover_matches:,}/{total_points:,} ({(land_cover_matches/total_points)*100:.1f}%)\")\n",
    "print(f\"   Points sans land cover : {total_points - land_cover_matches:,} ({((total_points - land_cover_matches)/total_points)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f917d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAITEMENT DES POINTS SANS MATCH AVEC KD-TREE\n",
      "   • Points sans match après buffer : 38,211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anfel\\AppData\\Local\\Temp\\ipykernel_9156\\2970428372.py:16: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  land_centroids = gdf_land.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • Construction du KDTree sur les centroïdes...\n",
      "   • Recherche des plus proches polygones...\n",
      "   • Distance moyenne: 0.057897° (6.43 km)\n",
      "   • Distance max: 1.086991° (120.66 km)\n",
      "   • Points complétés avec KDTree : 38,211\n",
      "\n",
      " RÉSULTATS FINAUX (BUFFER + KD-TREE) :\n",
      "   • Dataset shape : (56864, 19)\n",
      "   • Points avec land cover : 56,864 (100.0%)\n",
      "   • Points sans land cover : 0\n",
      "   • Distance moyenne: 0.057897° (6.43 km)\n",
      "   • Distance max: 1.086991° (120.66 km)\n",
      "   • Points complétés avec KDTree : 38,211\n",
      "\n",
      " RÉSULTATS FINAUX (BUFFER + KD-TREE) :\n",
      "   • Dataset shape : (56864, 19)\n",
      "   • Points avec land cover : 56,864 (100.0%)\n",
      "   • Points sans land cover : 0\n"
     ]
    }
   ],
   "source": [
    "# Pour les points sans match avec buffer, utiliser KDTree\n",
    "print(\"\\n TRAITEMENT DES POINTS SANS MATCH AVEC KD-TREE\")\n",
    "\n",
    "# Identifier les points sans land cover après le buffer\n",
    "mask_no_match = df_full['GRIDCODE'].isna()\n",
    "points_sans_match = mask_no_match.sum()\n",
    "\n",
    "print(f\"   • Points sans match après buffer : {points_sans_match:,}\")\n",
    "\n",
    "if points_sans_match > 0:\n",
    "    # Extraire les coordonnées des points sans match\n",
    "    gdf_no_match = gdf_final[mask_no_match].copy()\n",
    "    no_match_coords = gdf_no_match[['lon', 'lat']].to_numpy()\n",
    "    \n",
    "    # Extraire les centroïdes des polygones land cover\n",
    "    land_centroids = gdf_land.geometry.centroid\n",
    "    land_coords = np.array([[geom.x, geom.y] for geom in land_centroids])\n",
    "    \n",
    "    # Construire le KDTree sur les centroïdes\n",
    "    print(\"   • Construction du KDTree sur les centroïdes...\")\n",
    "    tree = cKDTree(land_coords)\n",
    "    \n",
    "    # Trouver le polygone le plus proche pour chaque point sans match\n",
    "    print(\"   • Recherche des plus proches polygones...\")\n",
    "    dist, idx = tree.query(no_match_coords, k=1)\n",
    "    \n",
    "    print(f\"   • Distance moyenne: {dist.mean():.6f}° ({dist.mean() * 111:.2f} km)\")\n",
    "    print(f\"   • Distance max: {dist.max():.6f}° ({dist.max() * 111:.2f} km)\")\n",
    "    \n",
    "    # Récupérer les attributs land cover correspondants\n",
    "    land_attrs = gdf_land[['GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']].iloc[idx.flatten()].reset_index(drop=True)\n",
    "    \n",
    "    # Mettre à jour df_full pour les points sans match\n",
    "    df_full.loc[mask_no_match, 'GRIDCODE'] = land_attrs['GRIDCODE'].values\n",
    "    df_full.loc[mask_no_match, 'log_area_sqm'] = land_attrs['log_area_sqm'].values\n",
    "    df_full.loc[mask_no_match, 'lcc_code_encoded'] = land_attrs['lcc_code_encoded'].values\n",
    "    \n",
    "    # Ajouter la colonne de distance pour traçabilité\n",
    "    df_full['distance_to_landcover_deg'] = np.nan\n",
    "    df_full.loc[mask_no_match, 'distance_to_landcover_deg'] = dist\n",
    "    \n",
    "    print(f\"   • Points complétés avec KDTree : {points_sans_match:,}\")\n",
    "else:\n",
    "    print(\"   • Tous les points ont trouvé un match avec le buffer !\")\n",
    "\n",
    "# Statistiques finales\n",
    "print(f\"\\n RÉSULTATS FINAUX (BUFFER + KD-TREE) :\")\n",
    "print(f\"   • Dataset shape : {df_full.shape}\")\n",
    "print(f\"   • Points avec land cover : {df_full['GRIDCODE'].notna().sum():,} ({(df_full['GRIDCODE'].notna().sum()/len(df_full))*100:.1f}%)\")\n",
    "print(f\"   • Points sans land cover : {df_full['GRIDCODE'].isna().sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210086b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset sauvegardé : C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\local_dataset\\dataset\\buffer_kdtree.parquet\n",
      "   • Shape finale : (56864, 18)\n",
      "   • Colonnes : ['lon', 'lat', 'fire', 'log_precip_s1', 'log_precip_s2', 'log_precip_s3', 'log_precip_s4', 'tmax_s1', 'tmax_s2', 'tmax_s3', 'tmax_s4', 'amplitude_thermique_s1', 'amplitude_thermique_s2', 'amplitude_thermique_s3', 'amplitude_thermique_s4', 'GRIDCODE', 'log_area_sqm', 'lcc_code_encoded']\n"
     ]
    }
   ],
   "source": [
    "# Supprimer la colonne de distance si non nécessaire (optionnel)\n",
    "df_full = df_full.drop(columns=['distance_to_landcover_deg'], errors='ignore')\n",
    "\n",
    "# Enregistrer le dataset final en format Parquet\n",
    "output_path = r\"C:\\Users\\anfel\\OneDrive\\Desktop\\M2\\prjt\\local_dataset\\dataset\\buffer_kdtree.parquet\"\n",
    "\n",
    "df_full.to_parquet(output_path, engine='fastparquet', compression='snappy')\n",
    "\n",
    "print(f\"\\n Dataset sauvegardé : {output_path}\")\n",
    "print(f\"   • Shape finale : {df_full.shape}\")\n",
    "print(f\"   • Colonnes : {list(df_full.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
