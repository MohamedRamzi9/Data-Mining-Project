% \documentclass{article}
% \usepackage{graphicx}
% \usepackage{float}

% \begin{document}


\subsection{Random Forest}

\subsubsection{Definition}
\textbf{Random Forest} est un algorithme d'apprentissage supervisé utilisé pour la classification et la régression. Il fonctionne en combinant plusieurs arbres de décision construits sur des sous-échantillons aléatoires des données, et en agrégeant leurs prédictions pour améliorer la précision et réduire le surapprentissage. Chaque arbre de décision est construit en sélectionnant aléatoirement un sous-ensemble des caractéristiques à chaque nœud, ce qui introduit de la diversité parmi les arbres et améliore la robustesse du modèle global.

Les critères utilisés pour mesurer la qualité des divisions incluent sans les meme que celle d'arbre de decision comme Gini, le gain d'information (entropie) et la réduction de la variance.

Les paramètres clés de l'algorithme Random Forest incluent :
\begin{itemize}
    \item tous les paramètres de l'arbre de décision, tels que :
    \begin{itemize}
        \item \textbf{Max Depth :} La profondeur maximale de l'arbre, généralement utilisé pour contrôler la complexité de l'arbre et éviter le surapprentissage.
        \item \textbf{Min Samples Split :} Le nombre minimum d'échantillons requis pour diviser un nœud, généralement fixé à la valeur minimale de 2.
        \item \textbf{Min Samples Leaf :} Le nombre minimum d'échantillons requis pour être à une feuille, généralement fixé à la valeur minimale de 1.
        \item \textbf{Criterion :} La fonction utilisée pour mesurer la qualité des divisions (par exemple, Gini, Entropie).
    \end{itemize}
    \item \textbf{N Estimators :} Le nombre d'arbres dans la forêt, varie selon le dataset.
\end{itemize}

\subsubsection{Implemenation from scratch}
Voici notre implémentation de l'algorithme Random Forest en Python.

\begin{verbatim}
Implemenation Random Forest from scratch
\end{verbatim}


\subsection{Normalization et Encodage de données}
Puisque Random Forest est basé sur des arbres de décision, nous avons appliqué les mêmes techniques de normalisation et d'encodage que pour l'algorithme d'arbre de décision : 
\begin{itemize}
    \item Pour l'encodage des caractéristiques categorilaes, nous avons utilisé \textbf{Ordinal Encoding} pour ne pas introduire beaucoup de nouvelle caractéristiques ainsi que pour preserver un order entre les valeurs pour permettre à l'algorithme de mieux séparer les données.
\end{itemize}

\subsection{Balancement de données}
Pour le balancement de classes comme avec les arbres de décision, nous avons utilisé un model de Random Forest de base avec les paramètres importants fixés à 50 pour \textbf{n\_estimators} et "gini" pour \textbf{criterion} et nous avons utilisé la métrique \textbf{ROC AUC} pour l'évaluation. Les techniques de balancement que nous avons testées sont :
\begin{itemize}
    \item \textbf{Original Data :}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{random_forest/original.png}
        \caption{Random Forest avec données originales}
    \end{figure}
    \item \textbf{Random Under Sampling :} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{random_forest/random_under_sampling.png}
        \caption{Random Forest avec données random under sampling}
    \end{figure}

    \item \textbf{Ranodom Over Sampling :} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{random_forest/random_over_sampling.png}
        \caption{Random Forest avec données random over sampling}
    \end{figure}
    
    \item \textbf{Smote Over Sampling :}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{random_forest/smote_over_sampling.png}
        \caption{Random Forest avec données smote over sampling}
    \end{figure}
    
    \item \textbf{Tomek Under Sampling :} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{random_forest/tomek_under_sampling.png}
        \caption{Random Forest avec données tomek under sampling}
    \end{figure}
\end{itemize}
À la fin la meilleure technique de balancement pour Random Forest est \textbf{Random Over Sampling} avec les resultats suivants :
\begin{itemize}
    \item Best sampling method: Random Over-Sampling with ROC AUC: 0.9784
    \item New dataset sizes: Train=67819, Test=16955, Full=84774
    \item Class distribution: class 0: 33893, class 1: 33926
\end{itemize}

\subsection{Réglage des paramètres}
Pour le réglage des paramètres de l'algorithme Random Forest, nous avons utilisé la validation croisée (k-folds avec $k=2$) avec une recherche de grille, la plage des paramètres testés est la suivante :
\begin{itemize}
    \item \textbf{max\_depth, min\_samples\_split, min\_samples\_leaf :} les memes valeurs optimales que pour l'arbre de décision
    \item \textbf{n\_estimators :} [20, 30, 50, 100] le nombre d'arbres dans la forêt
    \item \textbf{max\_features :} [None, "sqrt", "log2"] le nombre de caractéristiques à considérer lors de la recherche de la meilleure division pour chaque arbre, None signifie que toutes les caractéristiques sont utilisées
    \item \textbf{criterion :} ["gini", "entropy"] la fonction pour mesurer la qualité d'une division dans chaque arbre
\end{itemize}
Voici le diagramme d'importance des caractéristiques pour le modèle Random Forest avec les meilleurs paramètres trouvés :
\begin{figure}[H][
    \centering
    \includegraphics[width=1\textwidth]{random_forest/feature_importance.png}
    \caption{Random Forest - Feature Importance}
\end{figure}
Les meilleurs paramètres trouvés sont sont illustrés dans Table~\ref{tab:random_forest-paramter-tuning-comparison}.


\subsection{Réduction de dimensionnalité}
Comme toujours, Random Forest peut bénéficier de la réduction de dimensionnalité puisque il est basé sur des arbres de décision. Nous avons utilisé l'analyse en composantes principales (PCA) pour réduire la dimensionnalité des données avec une variance expliquée de $90$\%. Nous avons eu le meme résultat que celui des arbres de decision (car le nombre de caractéristiques est le meme grace au meme type d'encodage). Après avoir appliqué PCA, le nombre de caractéristiques est passé de 38 à 1. Nous avons ensuite répété le processus de réglage des paramètres avec les mêmes plages de valeurs. Les meilleurs paramètres trouvés après la réduction de dimensionnalité sont également illustrés dans Table~\ref{tab:random_forest-paramter-tuning-comparison}.

\subsection{Comparaison de resultats}
Les resultats finaux obtenus avec KNN après le réglage des paramètres et l'utilisation de la technique de balancement Random Over Sampling sont les suivants :


\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Criterion} & \textbf{N Estimators}  & \textbf{Max Features} \\
\hline
Sans réduction de dimensionnalité & gini & 100 & log2 \\
Avec réduction de dimensionnalité & gini & 100 & log2 \\
\hline
\end{tabular}
\caption{Comparaison des performances avec et sans réduction de dimensionnalité}
\label{tab:random_forest-parameter-tunig-comparaison}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\hline
Sans réduction de dimensionnalité & 0.9985 & 0.9969 & 1 & 0.9985 \\
Avec réduction de dimensionnalité & 0.9434 & 0.9108 & 0.9829 & 0.9455 \\
\hline
\end{tabular}
\caption{Comparaison des performances avec et sans réduction de dimensionnalité}
\label{tab:random_forest-dim-reduction-comparison}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{random_forest/final_no_pca.png}
    \caption{Random Forest - Final Results with no PCA}
    \includegraphics[width=1\textwidth]{random_forest/final_with_pca.png}
    \caption{Random Forest - Final Results with PCA}
\end{figure}

On observe que la réduction de dimensionnalité a eu un impact négatif sur les performances de l'algorithme Random Forest dans ce cas, contrairement à l'arbre de décision. Cependant, elle a permis de réduire significativement le temps de calcul et les ressources nécessaires pour l'entraînement et la prédiction du modèle en réduisant tous les caractéristiques du dataset en une seule composante.
On remarque aussi que les paramètres optimaux restent les mêmes avant et après la réduction de dimensionnalité.

% \end{document}